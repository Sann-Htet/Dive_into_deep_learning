{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67616f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "from d2l import torch as d2l\n",
    "\n",
    "seed = 0 # Random number generator seed\n",
    "gamma = 0.95 # Discount factor\n",
    "num_iters = 10 # Number of iterations\n",
    "random.seed(seed) # Set the random seed to ensure results can be reproduced\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Now set up the environment\n",
    "env_info = d2l.make_env('FrozenLake-v1', seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e330a24",
   "metadata": {},
   "source": [
    "## Implementation of Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env_info, gamma, num_iters):\n",
    "    env_desc = env_info['desc'] # 2D array shows what each item means\n",
    "    prob_idx = env_info['trans_prob_idx']\n",
    "    nextstate_idx = env_info['nextstate_idx']\n",
    "    reward_idx = env_info['reward_idx']\n",
    "    num_states = env_info['num_states']\n",
    "    num_actions = env_info['num_actions']\n",
    "\n",
    "    mdp = env_info['mdp']\n",
    "    V = np.zeros((num_iters + 1, num_states))\n",
    "    Q = np.zeros((num_iters + 1, num_states, num_actions))\n",
    "    pi = np.zeros((num_iters + 1, num_states))\n",
    "    \n",
    "    for k in range(1, num_iters + 1):\n",
    "        for s in range(num_states):\n",
    "            for a in range(num_actions):\n",
    "                # Calculate \\sum_{s'} p(s'\\mid s,a) [r + \\gamma v_k(s')]\n",
    "                for pxrds in mdp[(s,a)]:\n",
    "                    # mdp(s,a): [(p1,next1,r1,d1),(p2,next2,r2,d2),..]\n",
    "                    pr = pxrds[prob_idx] # p(s'\\mid s,a)\n",
    "                    nextstate = pxrds[nextstate_idx] # Next state\n",
    "                    reward = pxrds[reward_idx] # Reward\n",
    "                    Q[k,s,a] += pr * (reward + gamma * V[k - 1, nextstate])\n",
    "            # Record max value and max action\n",
    "            V[k,s] = np.max(Q[k,s,:])\n",
    "            pi[k,s] = np.argmax(Q[k,s,:])\n",
    "    d2l.show_value_function_progress(env_desc, V[:-1], pi[:-1])\n",
    "\n",
    "value_iteration(env_info=env_info, gamma=gamma, num_iters=num_iters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
